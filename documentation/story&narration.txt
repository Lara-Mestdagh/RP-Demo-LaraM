Based on the setting certain characters are available, this way we don't end up with a mermaid in the desert or something similar.
Themes are not dependent on either setting or characters.

I installed Ollama and got a Llama 3.1 8B model running. This is generating pretty good stories in my opinion, I don't see a need to
fine-tune this model as that seems like it would be a waste of time and resources. It seems like the prompt engineering alone is sufficient.

I tested several models to see which ones make the best stories, my top 3 ended up being:
1. Phi 4
2. Qwen 2.5
3. Llama 3.1

But I still had the issue that the word count was too low for my liking, so next I tried to generate the stories based on a 3-act structure.
I use 3 different prompts and pass the previously generated responses as context as well, this way I hope to increase the overall
length of the stories and also have some more suspense and character growth throughout as well.

The length is definitely improved so far but I did some more tweaking with the prompts since some of the vocabulary used is a bit
too complex for my target audience. I also plan to use Llama 3.1 for this project since this has some of the faster inference time
and the resulting stories are decent.

Next was adding the narration part of the application, I decided to use meloTTS since it has a fast inference time with good
voice quality. I found a meloTTS API docker image and used this in my workflow. I also added the automatic evaluation for the story 
before it gets narrated, if there are any issues with the validation the story model is asked to re-generate the story until it 
passes all checks. This being mainly the readability of the story, the word count and lastly if there are any prohibited words found
in the story.

Next I created a simple Gradio interface where the user can select the metadata for the prompt, this then goes over the full pipeline:
First we create the story, if it is valid then the 3 separate parts are saved along with the full story text
Next we send the 3 story paths to the TTS model, which then creates 3 narration files with the beginning, middle and end
And next is the music generation, I decided to create 4 short music clips to put between each narration and at the very beginning and end.
The final step is combining all the audio files in the correct order and sending this back to the interface so it can be played on there.

This took some time to get working, I also created a small list of instruments by setting in order to keep consistency in the music generation.
During the combining, I noticed issues with the narration, reason being that the sample rate of this was higher than the music generated.
To fix this we upscale the music to the same sample rate as the TTS, which in my case was 44100 Hz.

